{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this tutorial is to introduce Spark, a handy and powerful distributed programming framework for large scale data processing. \n",
    "\n",
    "Let's first consider a practical problem: if you are given a text file, can you find out how many different words exist in that file? And what's their occurances? This won't be a hard problem for you as a CMU student! You can simply write a program to read the file into memory, split the text to get a collection of words, and iterate over it to record different words and their occurence, done! \n",
    "\n",
    "Then, what if you are given a corpus with many huge files that may contain millions of distinct words. Are you gonna to use the same way as described above to count the results? You probably cannot, consider the amount of data you will need to read into memory and the data structure you need to maintain as well as the time it takes to do this job in a single machine. Naturally, one solution would be to run this job using many machines at the same time to make use of parallelism and utilize the resources provided by these machines. But here comes the problem, you will need to handle many problems under this distributed computing context (e.g. task allocation, consistency, monitoring, machine communication...) which make this job no longer trivial and your life harder.\n",
    "\n",
    "Don't worry! The Spark programming framework I introduce in this tutorial is designed to deal with such problems for you as a programmer. It provides a clean interface and allow you to build a distributed program without the need to worry about all the complicated distributed programming details.\n",
    "\n",
    "Spark is very useful in machine learning and data science in many different scenarios including data preprocessing, feature engineering, model training and model inferencing. Spark can be deployed in large data center to utilize the resources to speed up computing. It's easy to use and is a very popular framework nowadays. It'll be very of great helpful if you know this framework and can apply Spark in your daliy work as a data science practitioner.\n",
    "\n",
    "### Tutorial Content\n",
    "To write a spark program, the programming language you can choose from including [scala](https://www.scala-lang.org/), [java](http://www.oracle.com/technetwork/java/javase/downloads/index.html), [python](https://www.python.org/) and [R](https://www.r-project.org/). This tutorial will only use python to demonstrate how spark works since the underlying ideas are the same regardless of the what programming language you are using. In addition, this tutorial will use data preprocessing on Spark as an example for the sake of simplicity and easy understand, so that you can have the intuitive about the details of Spark programming framework. But you can definitely make use of Spark in other tasks like iterative machine learning algorithms.\n",
    "\n",
    "The remaining part of this tutorial is organized as the following sections:\n",
    "\n",
    "- [Environment setup and configuration](#Environment-setup-and-configuration)\n",
    "- [Spark programming framework](#Spark-programming-framework)\n",
    "- [Rewrite Feature Generating Problem in HW3](#Rewrite Feature Generating Problem in HW3)\n",
    "- [Where to go from here]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before write any code, you will need to have Spark installed. Normally, Spark is configured in large computer clusters but it also support running locally for the purpose of learning and debugging. So in this tutorial, you will configure Spark on your own PC and run it locally to learn how to use Spark since there is no difference between writing a Spark program that run in a cluster or run locally. However, of course, all the distributed operations are actually running on your single machine while running locally.\n",
    "\n",
    "#### Download Spark\n",
    "\n",
    "First, follow the instructions and download [Spark](https://spark.apache.org/downloads.html). I would recommand using pip to install Spark if you could:\n",
    "\n",
    "    $ pip install pyspark\n",
    "    \n",
    "Pyspark is the Spark library for python, this command will download pyspark and automatically configure the environment which can save you lots of time. Also you are free to download the compressed package. Since Spark use the Hadoop Distributed File System as the underlying file system, it will need the support from [Hadoop](http://hadoop.apache.org/) (It is an open-source implementation of MapReduce framework, you do not need to know about this). You can download a pre-built package with Hadoop included or you can configure Spark with your own Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issue related to java\n",
    "\n",
    "Spark use java virtual machine to run tasks, so it requires the support from java. Although from the document of the latest version of [Spark](https://spark.apache.org/docs/latest/), it has the support for java 9. But from what I observed on my own computer, Spark may has some errors while running on java 9. So I would recommand you to use [java 8](http://www.oracle.com/technetwork/java/javase/downloads/index.html) to run Spark. If you already has java 9 installed, you can first download java 8 and then use [jenv](http://www.jenv.be/) to configure multiple versions of java on your system, and switch to java 8 while running Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark programming framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I'll introduce and explain the basic idea about Spark programming framework. First, I will explain some basic concepts by walking through a simple Spark program first.\n",
    "\n",
    "1. RDD\n",
    "2. Transformation & Action\n",
    "3. Partitions in distributed mode (parallel)\n",
    "4. In memory cache & cache trick & when to release it\n",
    "5. Failure handling in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark # Import the pyspark module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resilient Distributed Dataset\n",
    "The programming interface provided by Spark is called *Resilient Distributed Dataset (RDD)*, it's an in-memory data representation format that store data across multiple nodes in one cluster. To understand RDD, yuo can think it as a list and each element in RDD is a list entry which is also known as record. When you invoke an operation on RDD, it will perform on each record of this RDD, which is similar to the apply method of dataframe. To create an RDD, you can either invoke the *parallelize()* method and transform an existing python collection into RDD (usually used to create the very first RDD) or read data into the driver program from external sources (e.g. files, database) as RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Initialize sparkContext, configuration parameters can be set at this time using SparkConf.\n",
    "# Or you can configure through a configuration file.\n",
    "sc_conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=sc_conf)\n",
    "# 2. Load data into Spark program. Imagine the words are read from a corpus :-)\n",
    "word_list = ['Data', 'Science', 'Science', 'Practical', 'Data', 'Fun', 'is', 'Fun']\n",
    "# 3. Using parallelize to transfer word_list into a spark RDD.\n",
    "word_list_rdd = sc.parallelize(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Spark application consist of a master node and several slave nodes. There is one driver program running on the master ndoe which can be regarded as the program entrance point. It will create the first RDD, invoke operations on it and collect results from cluster. The driver program is what you are going to write when develop a distributed program in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation & Action\n",
    "There are two kinds of operation based on the RDD programming interface: **transformation** and **action**.\n",
    "\n",
    "- Transformation will generate a new RDD based on the one that invoke this transformation. **Transformations are lazy**: it won't compute right away. Instead, it will record the transformation applied to this RDD and the actual computation will only happen when an action is invoked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. Map & Reduce \"transformation\" to get the count of different words in the input.\n",
    "word_list_key_value_rdd = word_list_rdd.map(lambda x: (x, 1))\n",
    "result_rdd = word_list_key_value_rdd.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the narrow programming interface provided by Spark, we cannot directly use *word_list_rdd* to count the occurance of different word. So, the above code creates two RDD by invoking map and reduceByKey function on existing RDD to get the counting result. It can be demonstrated with the following figure:![https://raw.githubusercontent.com/Albertle/15688_Tutorial/master/figure/First_Example.jpg](https://raw.githubusercontent.com/Albertle/15688_Tutorial/master/figure/First_Example.jpg)\n",
    "1. First, by invoking the *map* function, each record in *word_list_rdd* is mapped to a key-value tuple of the form (word, 1), where word is the key with value of 1, and a new RDD *word_list_key_value_rdd* is generated.\n",
    "2. Then, the *reduceByKey* will perform operation on all records with the same key. And here, it just add up the value of all record tuples with the same key to get the count of each words.\n",
    "\n",
    "The lambda expression passed into these two operations can be regarded as a easy way to define function. More about [lambda expression](http://www.secnetix.de/olli/Python/lambda_functions.hawk) if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Action will cause all transformation related to an RDD to actually compute and bring the results back to the driver program: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Science', 2), ('Practical', 1), ('Fun', 2), ('is', 1), ('Data', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Collect \"action\" to actually run all the transformation related to result_rdd before\n",
    "# and bring the result to master.\n",
    "result = result_rdd.collect()\n",
    "print(result)\n",
    "# 6. Stop the sparkContext created before.\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: [('Science', 2), ('Practical', 1), ('Fun', 2), ('is', 1), ('Data', 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitions\n",
    "One core idea of Spark is partition. Let's take a look at the following code snippet to understand what is partition of RDD. It's a crawler program that get the html page of all address in the *crawler_address.txt* which are:\n",
    "- https://www.cs.cmu.edu/~15719/index.html\n",
    "- https://www.cmu.edu/\n",
    "- http://store.steampowered.com/\n",
    "- http://www.datasciencecourse.org/\n",
    "- http://www.bbc.com/\n",
    "- https://www.cmu.edu/ini/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Crawler function that receive a url, and return the content of the webpage.\n",
    "def crawler(address):\n",
    "    res = requests.get(address)\n",
    "    return res.text\n",
    "\n",
    "# 1. Initialize sparkContext and pass in default configuration.\n",
    "sc = pyspark.SparkContext(conf=pyspark.SparkConf())\n",
    "# 2. Read the file into Spark as an RDD. Each record of this RDD is a line in the file.\n",
    "# The number 3 tell Spark to divide this RDD into 3 partitions, and each contains 2 url.\n",
    "address_rdd = sc.textFile(\"crawler_address.txt\", 3)\n",
    "# 3. \"Map\" each http address into the corresponding html page.\n",
    "crawler_result_rdd = address_rdd.map(crawler)\n",
    "# 4. Trigger all computations and bring result back to driver program.\n",
    "final_res = crawler_result_rdd.collect()\n",
    "print(len(final_res)) # Output: 6\n",
    "print(final_res) # Output: a list containing the web content of each page\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may observe there are some slight differences between this snippet and the previous one:\n",
    "\n",
    "1. Here, the method *textFile* is used to create the very first RDD. It create an RDD by loading a file, and each file line is a record in the generated RDD.\n",
    "2. *textFile* method has a second argument which is passed in 3. This second argument is **partition number** and is to tell Spark to break down this RDD into 3 partitions, so that each partition contains two urls. Each partition can be executed in different machines in parallel and this is why Spark can execute some very large tasks in a timely efficient manner. How many partition an RDD should have is totally up to you, however, you need to make sure you have enough Spark slave set up to execute these partitions, or parallelism won't be fully utilized. Do note that many other transformation take partition number as an optional argument.\n",
    "3. The input argument of *map* is the function *crawler*, which takes one url as argument. *Map* function will pass each record in each partition of this RDD into *crawler()* to get the web content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lineage of RDD & Caching\n",
    "The lineage of RDD is the key to ensure the lazy characteristic of transformations. For instance, in the first example, the lineage of RDD can be depicted as:\n",
    "\n",
    "    word_list_rdd <==== word_list_key_value_rdd <=== result_rdd <==== collect() (The direction of arrow indicate dependency.)\n",
    "\n",
    "This lineage is built when transformation is invoked to create RDDs, but actual computation doesn't happen. And when an action is invoked on *result_rdd*, the transformation chain is then actually computed to create corresponding RDDs.\n",
    "\n",
    "There is one caveat here: when an RDD is computed in one lineage chain and is used by the following transformation, this RDD will **not** be stored in memory or write to disk by default. Instead, if you want to store a computed RDD, you need to explicitly invoke *persist()* or *cache()* function. The following example demonstrate how to cache an RDD, it's very similar to the first example, except it perform one more action on RDD *word_list_key_value_rdd* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Science', 2), ('Practical', 1), ('Fun', 2), ('is', 1), ('Data', 2)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "sc = pyspark.SparkContext(conf=pyspark.SparkConf())\n",
    "word_list = ['Data', 'Science', 'Science', 'Practical', 'Data', 'Fun', 'is', 'Fun']\n",
    "word_list_rdd = sc.parallelize(word_list, 4)\n",
    "word_list_key_value_rdd = word_list_rdd.map(lambda x: (x, 1))\n",
    "# 1. Persist this RDD after it's computed.\n",
    "word_list_key_value_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "result_rdd = word_list_key_value_rdd.reduceByKey(lambda x, y: x + y)\n",
    "result = result_rdd.collect()\n",
    "print(result)\n",
    "# 2. Here saveAsTextFile is another “action”, which will save this rdd into\n",
    "# local file system, where each rdd entry will be a file line.\n",
    "# Note: you need to make sure the folder 'key_value_output' does not already exist\n",
    "# in this working directory.\n",
    "word_list_key_value_rdd.saveAsTextFile('key_value_output')\n",
    "# 3. Unpersist this RDD when no other computation is depend on it.\n",
    "word_list_key_value_rdd.unpersist()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Result: [('Science', 2), ('Practical', 1), ('Fun', 2), ('is', 1), ('Data', 2)]\n",
    "\n",
    "After executing *saveAsTextFile*, a folder named *key_value_output* will be created in your local file system, and five files will be created: ![https://raw.githubusercontent.com/Albertle/15688_Tutorial/master/figure/Third_Example.jpg](https://raw.githubusercontent.com/Albertle/15688_Tutorial/master/figure/Third_Example.jpg)\n",
    "Each part-xxxxx file will contains 2 record of this RDD.\n",
    "\n",
    "In this case, there are actually two lineage chain:\n",
    "    \n",
    "    word_list_rdd <==== word_list_key_value_rdd <=== result_rdd <==== collect()\n",
    "    word_list_rdd <==== word_list_key_value_rdd <=== saveAsTextFile()\n",
    "So there are two dependency upon *word_list_key_value_rdd*. When *collect()* is invoked on *result_rdd*, *word_list_key_value_rdd* is computed due to the first lineage. After that, another action *saveAsTextFile* is also depend on *word_list_key_value_rdd*, and by default, this RDD is not stored which means Spark will compute it again. \n",
    "\n",
    "However, in the code we invoked *persist(StorageLevel.MEMORY_AND_DISK)* on *word_list_key_value_rdd* which will explicitly make Spark to store this RDD after it's computed, so reuse of this RDD won't cause the same RDD to be computed again. There are several level of persistency which can be found [here](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence), and *cache()* is equivalent to *persist(StorageLevel.MEMORY_ONLY)*\n",
    "\n",
    "Since transformation is lazy, persist operation is also lazy. It will persist an RDD accordingly when the RDD is actually computed. So invocation of *persist()* on an RDD should be done before its computation happen. On the contrary, *unpersist()*, which will free the space used to store an RDD, will be executed at the point it's invoked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failure Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lineage of RDD also ensure the robustness of Spark. When failure happens and some computed RDDs are lost, Spark can recompute them automatically with the lineage information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite Feature Generating Problem in HW3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the linear regression problem of HW3, we try to predict the time when a bus will arrive at a bus stop. The very first step we did is to do a basic feature engineering of the bus data. In this section, I will show how to rewrite the HW problem into a distributed Spark program.\n",
    "\n",
    "I store part of (about 70000+ records, due to the submission limit) the trip data into *labeled_vdf.csv* file. For your convenience, the data has the format: ![https://raw.githubusercontent.com/Albertle/15688_Tutorial/master/figure/Fourth_Example.jpeg](https://raw.githubusercontent.com/Albertle/15688_Tutorial/master/figure/Fourth_Example.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the program will read the csv file to load data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70106\n",
      "{'tmstmp': '8/11/16 16:11', 'vid': '3202', 'lat': '40.44067764', 'lon': '-80.00022125', 'hdg': '114', 'pid': '4521', 'rt': '61A', 'des': 'Swissvale', 'pdist': '118', 'spd': '18', 'tablockid': '061A-178', 'tatripid': '6759', 'eta': '27'}\n"
     ]
    }
   ],
   "source": [
    "source_data_list = list()\n",
    "header = list()\n",
    "# 1. Load data into program. Each line will be read as a dictionary of fields and appended to a list.\n",
    "with open('labeled_vdf.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    header = csv_reader.__next__()\n",
    "    for row in csv_reader:\n",
    "        row_dict = dict()\n",
    "        for i in range(0, len(header)):\n",
    "            row_dict[header[i]] = row[i]\n",
    "        source_data_list.append(row_dict)\n",
    "print(len(source_data_list))\n",
    "print(source_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, write the function that will generate feture for each of the record passed in as what you did in HW3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Main logic of map function.\n",
    "# Input is a record line, which is a dictionary of fields.\n",
    "# Return will also be a dictionary of generated features.\n",
    "def generate_feature(record):\n",
    "    feature_dict = dict()\n",
    "    # a. Retain the numerical feature\n",
    "    numerical_feature = ['pdist', 'spd', 'lat', 'lon', 'eta']\n",
    "    for name in numerical_feature:\n",
    "        feature_dict[name] = record[name]\n",
    "    # b. Adding bias\n",
    "    feature_dict['bias'] = 1\n",
    "    # c. Create time format\n",
    "    pi_2 = 2 * math.pi\n",
    "    date = datetime.strptime(record['tmstmp'], '%m/%d/%y %H:%M')\n",
    "    feature_dict['sin_day_of_week'] = math.sin((date.weekday() / 7 ) * pi_2)\n",
    "    feature_dict['cos_day_of_week'] = math.cos((date.weekday() / 7 ) * pi_2)\n",
    "    \n",
    "    feature_dict['sin_hour_of_day'] = math.sin((date.hour / 24) * pi_2)\n",
    "    feature_dict['cos_hour_of_day'] = math.cos((date.hour / 24) * pi_2)\n",
    "    \n",
    "    feature_dict['sin_time_of_day'] = math.sin(((date.hour * 60 + date.minute) / (60 * 24)) * pi_2)\n",
    "    feature_dict['cos_time_of_day'] = math.cos(((date.hour * 60 + date.minute) / (60 * 24)) * pi_2)\n",
    "    \n",
    "    # d. Heading\n",
    "    feature_dict['sin_hdg'] = math.sin((float(record['hdg']) / 360) * pi_2)\n",
    "    feature_dict['cos_hdg'] = math.cos((float(record['hdg']) / 360) * pi_2)\n",
    "    \n",
    "    # e. Weekday\n",
    "    if date.weekday() < 5:\n",
    "        feature_dict['weekday'] = 1\n",
    "    else:\n",
    "        feature_dict['weekday'] = 0\n",
    "    \n",
    "    # f. Destination\n",
    "    destination_feature = ['Braddock', 'Downtown', 'Greenfield Only', 'McKeesport', 'Murray-Waterfront', 'Swissvale']\n",
    "    for des in destination_feature:\n",
    "        if record['des'] == des:\n",
    "            feature_dict[des] = 1\n",
    "        else:\n",
    "            feature_dict[des] = 0\n",
    "\n",
    "    # g. Round\n",
    "    rt_feature = ['61A', '61B', '61C', '61D']\n",
    "    for rt in rt_feature:\n",
    "        if record['rt'] == rt:\n",
    "            feature_dict[rt] = 1\n",
    "        else:\n",
    "            feature_dict[rt] = 0\n",
    "    return feature_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the main Spark program to do feature engineering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70106\n",
      "{'pdist': '118', 'spd': '18', 'lat': '40.44067764', 'lon': '-80.00022125', 'eta': '27', 'bias': 1, 'sin_day_of_week': 0.43388373911755823, 'cos_day_of_week': -0.900968867902419, 'sin_hour_of_day': -0.8660254037844384, 'cos_hour_of_day': -0.5000000000000004, 'sin_time_of_day': -0.8890171414857364, 'cos_time_of_day': -0.45787391511695663, 'sin_hdg': 0.913545457642601, 'cos_hdg': -0.4067366430758001, 'weekday': 1, 'Braddock': 0, 'Downtown': 0, 'Greenfield Only': 0, 'McKeesport': 0, 'Murray-Waterfront': 0, 'Swissvale': 1, '61A': 1, '61B': 0, '61C': 0, '61D': 0}\n",
      "dict_keys(['pdist', 'spd', 'lat', 'lon', 'eta', 'bias', 'sin_day_of_week', 'cos_day_of_week', 'sin_hour_of_day', 'cos_hour_of_day', 'sin_time_of_day', 'cos_time_of_day', 'sin_hdg', 'cos_hdg', 'weekday', 'Braddock', 'Downtown', 'Greenfield Only', 'McKeesport', 'Murray-Waterfront', 'Swissvale', '61A', '61B', '61C', '61D'])\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(conf=pyspark.SparkConf())\n",
    "# Transform into Spark RDD with 10 partitions.\n",
    "source_rdd = sc.parallelize(source_data_list, 10)\n",
    "# Invoke map to generate feature for each RDD record.\n",
    "feature_rdd = source_rdd.map(generate_feature)\n",
    "# Invoke action and bring result back to driver program.\n",
    "res = feature_rdd.collect()\n",
    "print(len(res))\n",
    "print(res[0])\n",
    "print(res[0].keys())\n",
    "# Save feature to csv file\n",
    "with open('feature.csv', 'w') as output:\n",
    "    writer = csv.DictWriter(output, res[0].keys())\n",
    "    writer.writeheader()\n",
    "    for row in res:\n",
    "        writer.writerow(row)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of the generated *feature.csv* file:![https://raw.githubusercontent.com/Albertle/15688_Tutorial/master/figure/Fourth_Example_2.jpeg](https://raw.githubusercontent.com/Albertle/15688_Tutorial/master/figure/Fourth_Example_2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we transform an ordinary program into a distributed Spark program. And if you run the above code in a real cluster, different partitions of the task will be executed in parallel on different machines to utilize the resources and speed up computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to go from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation! Now you are equipped with enough knowledge about Spark programming framework and its core idea. If you want to learn more, here are some materials I highly recommended you to look at:\n",
    "- [Official RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html): provide much more detailed information on RDD programming.\n",
    "- [Pyspark API](https://spark.apache.org/docs/latest/api/python/pyspark.html)\n",
    "- [MapReduce: Simplified Data Processing on Large Clusters](https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf): Google's paper on MapReduce framework. Since Spark is an implementation of MapReduce, it's helpful for you to understand programming framework of Spark.\n",
    "- [Spark: Cluster Computing with Working Sets](https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf): Paper published by Spark designer, more detailed design aspect of Spark is demonstrated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
